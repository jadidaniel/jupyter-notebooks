{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U.S.A. Presidential Vocabulary\n",
    "\n",
    "Whenever a United States of America president is elected or re-elected, an inauguration ceremony takes place to mark the beginning of the president’s term. During the ceremony, the president gives an inaugural address to the nation, dictating the tone and focus of the next four years of leadership.\n",
    "\n",
    "In this project you will have the chance to analyze the inaugural addresses of the presidents of the United States of America, as collected by the Natural Language Toolkit, using word embeddings.\n",
    "\n",
    "By training sets of word embeddings on subsets of inaugural address versus the collection of presidents as a whole, we can learn about the different ways in which the presidents use language to convey their agenda.\n",
    "\n",
    "\n",
    "\n",
    "Preprocessing the Data\n",
    "1.\n",
    "\n",
    "Provided in the workspace are .txt files for each of the 58 U.S.A. presidential inaugural addresses from 1789 to 2017. The files are named with the following convention:\n",
    "\n",
    "YEAR-PRESIDENT.txt\n",
    "\n",
    "where YEAR is the year the inaugural address was given and PRESIDENT is the name of the president who gave the address. If there is only one president with a given last name, only the last name is used. For presidents with the same last name, first name and middle initial(s), separated by a -, are included.\n",
    "\n",
    "Open the file navigator and click on a few of the files to view them.\n",
    "2.\n",
    "\n",
    "Navigate back to script.py. At the top of the file we have imported some libraries that will be helpful for your analysis, as well as some helper functions from the file president_helper.py.\n",
    "\n",
    "In order to create word embeddings on the corpus of all the presidents’ speeches, we need to read the text data from each file, separate the files into sentences on a word by word basis, and then merge all the sentences across the speeches into one big list of lists.\n",
    "\n",
    "Let’s start by finding all the file names for the .txt files we will be analyzing. Paste the following code into script.py to store a list of all .txt files in a variable files.\n",
    "\n",
    "files = sorted([file for file in os.listdir() if file[-4:] == '.txt'])\n",
    "\n",
    "Print files to view the names of each inauguration speech file.\n",
    "\n",
    "The os package function listdir() returns a list of all files in a given directory (folder). In the provided code, we filter out all files that do not end with .txt and sort the file names using the sorted() function.\n",
    "\n",
    "Since there are 58 inaugural addresses, the length of files should be 58.\n",
    "3.\n",
    "\n",
    "Imported from president_helper.py is a function called read_file() which takes a file name as an argument and returns the opened file as one big string.\n",
    "\n",
    "Call read_file() on each file in files, and save all the resulting string versions to a list named speeches.\n",
    "4.\n",
    "\n",
    "Now that we have a list of speeches as strings, we need to breakdown the speeches into words on a sentence by sentence basis.\n",
    "\n",
    "For example, if we have the following sentences:\n",
    "\n",
    "\"The cat in the hat is sad. He didn't have a mouse.\"\n",
    "\n",
    "The processed data would look as follows:\n",
    "\n",
    "[[\"the\",\"cat\",\"in\",\"the\",\"hat\",\"is\",\"sad\"],[\"he\",\"didn't\",\"have\",\"a\",\"mouse\"]]\n",
    "\n",
    "We’ve imported the function process_speeches() from president_helper.py in order to perform this preprocessing for you. The function takes a list of strings as an argument and returns a list of lists. Each inner list represents one inaugural address and is a list of lists as well. Each inner list of the inaugural address list represents a sentence of that address, and each item in the sentence list is a word token in that sentence (see the hint for further explanation of this structure).\n",
    "\n",
    "Call process_speeches() with speeches as an argument and save the resulting list to processed_speeches.\n",
    "5.\n",
    "\n",
    "In order to build a custom set of word embeddings using gensim, we need to convert our data into a list of lists, where each inner list is a sentence and each item in the inner list is a word token.\n",
    "\n",
    "Currently, our speech data is in the following form:\n",
    "\n",
    "speeches = [[[\"monkeys\",\"eat\",\"bananas\"],[\"they\",\"hang\",\"from\",\"trees\"],...], [[\"penguins\",\"are\",\"cute\"],[\"they\",\"cannot\",\"fly\"],...],...]\n",
    "\n",
    "Where we have two speeches, one about monkeys and another about penguins. We want to remove the out layer of list so our data is as so:\n",
    "\n",
    "speeches = [[\"monkeys\",\"eat\",\"bananas\"],[\"they\",\"hang\",\"from\",\"trees\"],..., [\"penguins\",\"are\",\"cute\"],[\"they\",\"cannot\",\"fly\"],...]\n",
    "\n",
    "We’ve imported a function merge_speeches() from president_helper.py that takes a list of all our processed speeches and returns a list of lists where each inner list is a sentence and each item in the inner list is a word token.\n",
    "\n",
    "Call merge_speeches() on processed_speeches and save the result to all_sentences.\n",
    "\n",
    "Our data is now processed and ready for some analysis!\n",
    "All Presidents\n",
    "6.\n",
    "\n",
    "To get a better understanding of the data, let’s take a look at the most frequently used words across all the inaugural addresses.\n",
    "\n",
    "Imported from president_helper.py is a function most_frequent_words(), which uses the Counter() function from Python’s collections module to find the most common words in a list of lists, where each item in the inner list is a word token.\n",
    "\n",
    "Call most_frequent_words() with all_sentences as an argument and save the result to most_freq_words. Print most_freq_words to the terminal.\n",
    "\n",
    "Once you have taken a look at the list of the commonly used words, you can comment out your print statement so your output terminal doesn’t display the words each time you run your code.\n",
    "7.\n",
    "\n",
    "Finally, it’s word embedding time! Create a word embedding model with gensim using the following function and keyword arguments:\n",
    "\n",
    "gensim.models.Word2Vec(__________, size=96, window=5, min_count=1, workers=2, sg=1)\n",
    "\n",
    "Save the model to a variable named all_prez_embeddings.\n",
    "\n",
    "Replace the __________ with your variable containing all the inaugural address sentences.\n",
    "8.\n",
    "\n",
    "Now that we have our word embeddings, let’s have some fun exploring them! The concept of “freedom” is prevalent in the speeches made by the presidents. Find the top 20 words that are used in similar contexts to “freedom”, and save the results to a variable named similar_to_freedom.\n",
    "\n",
    "Print similar_to_freedom to the terminal.\n",
    "9.\n",
    "\n",
    "What other words in the corpus of inaugural addresses do you want to analyze? Pick a word from most_freq_words and find other words that are used similarly. Are you surprised by the words that are used in the same contexts?\n",
    "One President\n",
    "10.\n",
    "\n",
    "A fun aspect of word embeddings is to see how different corpora result in different word embeddings, alluding to differences in how words are used between writers/authors/speakers.\n",
    "\n",
    "Let’s train a word embedding model on a single president and see how their word embeddings differ from the collection of all presidents.\n",
    "\n",
    "Provided in script.py is a function get_president_sentences() that takes a president’s name as a string argument and returns a list of processed sentences from every inaugural address given by the president.\n",
    "\n",
    "Call get_president_sentences() with \"franklin-d-roosevelt\" as an argument and save the result to roosevelt_sentences.\n",
    "11.\n",
    "\n",
    "To get a better understanding of President Franklin D Roosevelt’s speeches, let’s take a look at the most frequently used words across his inaugural addresses.\n",
    "\n",
    "Call most_frequent_words() with roosevelt_sentences as an argument and save the result to roosevelt_most_freq_words. Print roosevelt_most_freq_words to the terminal.\n",
    "\n",
    "Once you have taken a look at the list of the commonly used words, you can comment out your print statement so your output terminal doesn’t display the words each time you run your code.\n",
    "12.\n",
    "\n",
    "Now that we have the sentences from President Roosevelt’s speeches, create another word embedding model with gensim, using the same keyword arguments as in the earlier word embedding model you created, trained on just the inaugural address sentences for President Roosevelt. Save the embeddings to a variable named roosevelt_embeddings.\n",
    "13.\n",
    "\n",
    "Like with our previous word embedding model, let’s explore roosevelt_embeddings! Find the top 20 words that are used in similar contexts to “freedom”, and save the results to a variable named roosevelt_similar_to_freedom.\n",
    "\n",
    "Print roosevelt_similar_to_freedom to the terminal.\n",
    "\n",
    "How do the words similar to “freedom” in President Roosevelt’s embeddings compare to the words similar to “freedom” in the embeddings of all the presidents’ speeches?\n",
    "\n",
    "Are there any surprises?\n",
    "\n",
    "To find the words with “similar” word vectors to a given word, you can use the gensim word embedding model method .most_similar(), as follows:\n",
    "\n",
    "similar_to_my_word = my_word_embedding_model.most_similar(\"my_word\", topn=20)\n",
    "\n",
    "Replace \"my_word\" with the word for which you want to find similar word embeddings.\n",
    "\n",
    "The results you see from calling roosevelt_embeddings.most_similar(\"freedom\", topn=20) are less than satisfying (many of the words with similar embeddings are stop words, or commonly used words that give little insight into topic or context). This is because even for President Roosevelt, who gave the largest number of inaugural addresses, there is not enough data to produce robust word embeddings.\n",
    "\n",
    "In the next section, we will explore looking at the speeches of multiple presidents to increase our corpus size and produce better word embeddings.\n",
    "Selection of Presidents\n",
    "14.\n",
    "\n",
    "You may have noticed that the results from roosevelt_embeddings.most_similar(\"freedom\", topn=20) are less than satisfying. This is because we were working with a limited dataset, producing less robust and generalizable word embeddings.\n",
    "\n",
    "Let’s increase our corpus size and find more defined word embeddings by training a word embedding model on the inaugural addresses of a collection of multiple presidents.\n",
    "\n",
    "Provided in script.py is a function get_presidents_sentences() that takes a list of multiple presidents’ names as an argument and returns a list of processed sentences from every inaugural address given by the group of presidents.\n",
    "\n",
    "Call get_presidents_sentences() with [\"washington\",\"jefferson\",\"lincoln\",\"theodore-roosevelt\"] as an argument and save the result to rushmore_prez_sentences.\n",
    "\n",
    "The function get_presidents_sentences() accomplishes most of the preprocessing work we performed using multiple functions in previous tasks.\n",
    "\n",
    "The four presidents featured on Mount Rushmore in Keystone, South Dakota are George Washington, Thomas Jefferson, Theodore Roosevelt, and Abraham Lincoln.\n",
    "15.\n",
    "\n",
    "To get a better understanding of President Washington, Jefferson, Lincoln, and Theodore Roosevelt’s speeches, let’s take a look at the most frequently used words across their inaugural addresses.\n",
    "\n",
    "Call most_frequent_words() with rushmore_prez_sentences as an argument and save the result to rushmore_most_freq_words. Print rushmore_most_freq_words to the terminal.\n",
    "\n",
    "Once you have taken a look at the list of the commonly used words, you can comment out your print statement so your output terminal doesn’t display the words each time you run your code.\n",
    "16.\n",
    "\n",
    "Now that we have the sentences from the presidents featured on Mount Rushmore, create another word embedding model with gensim, using the same keyword arguments as in the earlier word embedding model you created, trained on just the inaugural address sentences for these presidents. Save the embeddings to a variable named rushmore_embeddings.\n",
    "17.\n",
    "\n",
    "Like with our previous word embedding models, let’s explore rushmore_embeddings! Find words that are used in similar contexts to “freedom”, and save the results to a variable named rushmore_similar_to_freedom.\n",
    "\n",
    "How do the words similar to “freedom” in these presidents’ embeddings compare to the words similar to “freedom” in the embeddings of all the presidents’ speeches? And to just Franklin D Roosevelt’s?\n",
    "\n",
    "Are there any surprises?\n",
    "18.\n",
    "\n",
    "What other words in the corpus of Mount Rushmore presidents’ inaugural addresses do you want to analyze? Pick a word from rushmore_most_freq_words and find other words that are used similarly. Are you surprised by the words that are used in the same contexts?\n",
    "19.\n",
    "\n",
    "Who are your favorite presidents? Do you want to see how their choice of words compares to presidents you view less favorably? Or controversial presidents?\n",
    "\n",
    "Perhaps you want to analyze presidents by political party affiliation?\n",
    "\n",
    "Create a new word embedding model trained on a corpus of sentences from the speeches of a selection of presidents that you decide. Find the words used similarly to “freedom”, and explore the embeddings of other words in the corpus.\n",
    "\n",
    "How do the embeddings from your model compare to the other models you have built? What surprises do you find?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# president_helper.py\n",
    "\n",
    "import os\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "def read_file(file_name):\n",
    "  with open(file_name, 'r+', encoding='utf-8') as file:\n",
    "    file_text = file.read()\n",
    "  return file_text\n",
    "\n",
    "def process_speeches(speeches):\n",
    "  word_tokenized_speeches = list()\n",
    "  for speech in speeches:\n",
    "    sentence_tokenizer = PunktSentenceTokenizer()\n",
    "    sentence_tokenized_speech = sentence_tokenizer.tokenize(speech)\n",
    "    word_tokenized_sentences = list()\n",
    "    for sentence in sentence_tokenized_speech:\n",
    "      word_tokenized_sentence = [word.lower().strip('.').strip('?').strip('!') for word in sentence.replace(\",\",\"\").replace(\"-\",\" \").replace(\":\",\"\").split()]\n",
    "      word_tokenized_sentences.append(word_tokenized_sentence)\n",
    "    word_tokenized_speeches.append(word_tokenized_sentences)\n",
    "  return word_tokenized_speeches\n",
    "\n",
    "def merge_speeches(speeches):\n",
    "  all_sentences = list()\n",
    "  for speech in speeches:\n",
    "    for sentence in speech:\n",
    "      all_sentences.append(sentence)\n",
    "  return all_sentences\n",
    "\n",
    "def get_president_sentences(president):\n",
    "  files = sorted([file for file in os.listdir() if president.lower() in file.lower()])\n",
    "  speeches = [read_file(file) for file in files]\n",
    "  processed_speeches = process_speeches(speeches)\n",
    "  all_sentences = merge_speeches(processed_speeches)\n",
    "  return all_sentences\n",
    "\n",
    "def get_presidents_sentences(presidents):\n",
    "  all_sentences = list()\n",
    "  for president in presidents:\n",
    "    files = sorted([file for file in os.listdir() if president.lower() in file.lower()])\n",
    "    speeches = [read_file(file) for file in files]\n",
    "    processed_speeches = process_speeches(speeches)\n",
    "    all_prez_sentences = merge_speeches(processed_speeches)\n",
    "    all_sentences.extend(all_prez_sentences)\n",
    "  return all_sentences\n",
    "\n",
    "def most_frequent_words(list_of_sentences):\n",
    "  all_words = [word for sentence in list_of_sentences for word in sentence]\n",
    "  return Counter(all_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import spacy\n",
    "from president_helper import read_file, process_speeches, merge_speeches, get_president_sentences, get_presidents_sentences, most_frequent_words\n",
    "\n",
    "# get list of all speech files\n",
    "files = sorted([file for file in os.listdir() if file[-4:] == '.txt'])\n",
    "# print(files)\n",
    "\n",
    "# read each speech file\n",
    "speeches = ([read_file(f) for f in files])\n",
    "\n",
    "\n",
    "# preprocess each speech\n",
    "processed_speeches = process_speeches(speeches)\n",
    "\n",
    "#After calling process_speeches() with speeches as an argument, the data will be formatted such that:\n",
    "    # processed_speeches[0] represents the first inaugural address in processed_speeches.\n",
    "    # processed_speeches[0][0] represents the first sentence in the first inaugural address in processed_speeches.\n",
    "    # processed_speeches[0][0][0] represents the first word in the first sentence in the first inaugural address in processed_speeches.\n",
    "\n",
    "\n",
    "# merge speeches\n",
    "all_sentences = merge_speeches(processed_speeches)\n",
    "\n",
    "\n",
    "# view most frequently used words\n",
    "most_freq_words = most_frequent_words(all_sentences)\n",
    "# print(most_freq_words[:10])\n",
    "\n",
    "# create gensim model of all speeches\n",
    "all_prez_embeddings = gensim.models.Word2Vec(all_sentences, size=96, window=5, min_count=1, workers=2, sg=1)\n",
    "\n",
    "# view words similar to freedom\n",
    "similar_to_freedom = all_prez_embeddings.most_similar(\"freedom\", topn=20)\n",
    "print(similar_to_freedom)\n",
    "\n",
    "\n",
    "# get President Roosevelt sentences\n",
    "roosevelt_sentences = get_president_sentences(\"franklin-d-roosevelt\")\n",
    "\n",
    "\n",
    "# view most frequently used words of Roosevelt\n",
    "roosevelt_most_freq_words = most_frequent_words(roosevelt_sentences)\n",
    "# print(roosevelt_most_freq_words[:10])\n",
    "\n",
    "# create gensim model for Roosevelt\n",
    "roosevelt_embeddings = gensim.models.Word2Vec(roosevelt_sentences, size=96, window=5, min_count=1, workers=2, sg=1)\n",
    "\n",
    "# view words similar to freedom for Roosevelt\n",
    "roosevelt_similar_to_freedom = roosevelt_embeddings.most_similar(\"freedom\", topn=20)\n",
    "print(roosevelt_similar_to_freedom)\n",
    "\n",
    "# get sentences of multiple presidents\n",
    "rushmore_prez_sentences = get_presidents_sentences([\"washington\",\"jefferson\",\"lincoln\",\"theodore-roosevelt\"])\n",
    "\n",
    "\n",
    "# view most frequently used words of presidents\n",
    "rushmore_most_freq_words = most_frequent_words(rushmore_prez_sentences)\n",
    "# print(rushmore_most_freq_words[:10])\n",
    "\n",
    "# create gensim model for the presidents\n",
    "rushmore_embeddings = gensim.models.Word2Vec(rushmore_prez_sentences, size=96, window=5, min_count=1, workers=2, sg=1)\n",
    "\n",
    "# view words similar to freedom for presidents\n",
    "rushmore_similar_to_freedom = rushmore_embeddings.most_similar(\"freedom\", topn=20)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
